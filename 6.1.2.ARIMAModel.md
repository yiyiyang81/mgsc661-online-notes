## Purpose
Multi-hotel chain can use past data and occupancy rates of the locations to derive means and characteristics about the location, to build a regression model to decide location choices. 

It is worthy to look into ARIMA ARIMA(Autoregresive Integrated Moving Average Model) model for such task. 


## ARIMA Components
### Auto-Regressive (AR or p)
The auto-regressive(also called p) model part uses past values to predict future values, where the term "auto-regressive" refers to the process of regressing on past values. 

$$ Y_t = \sum_{1 \leq i \leq p} \phi_i Y_{t-i} + \epsilon_t$$
    
where:
- $p$ is the dimension of the auto-regressive model, which is the number of periods in the past that the model uses for forecasting
- $Y_t$ is the value at time $t$
- $\phi_i$ are the parameters of the model, the weights that are applied to the past values
- $\epsilon_t$ is the error term at time $t$

> $Y_t$ essentially is a linear regression on its own past values.

### Integrated (I or d)
This component(also called d) accounts for the differencing of the data to make it stationary.
$$ Y'_t = Y_t - Y_{t-1}$$

### Moving Average (MA or q)
The moving average(also called q) model part uses past forecast errors to predict future values.
$$ Y_t = \mu + \sum_{1 \leq i \leq q} \theta_i \epsilon_{t-i}+ \epsilon_t $$

where:
- $q$ is the dimension of the moving average, which is the number of periods in the past that the model uses for looking at the error term
- $\theta_i$ are the parameters of the model, the weights that are applied to the error terms
- $\mu$ is the mean of the $Y_t$ values, $E(Y_t)$


This model looks at the errors that the forecast is making and appropriately corrects for it in the next forecast. 

In sum, 
$p$ is the order of the AR model,
$d$ is the degree of differencing,
$q$ is the order of the MA model.

## Method
Understanding stationarity is crucial for time series data. Many models including ARIMA require the data to be stationary. 

### Stationarity
A time series is considered stationary if its statistical properties such as mean, variance, and autocorrelation are constant over time. This implies that the data has no trend, seasonality or varying volatility over time.

A stock price might have a trend in its pricing, but may be relatively stationary in the difference in price. We can apply differencing to make the data stationary. 

> autocorrelation is the correlation between the current value of the time series and its lagged version over time.

> Differencing is the process of subtracting the previous observation from the current observation.

Depending on the need we may apply differencing multiple times(1st order, 2nd order, etc.) to make the data stationary.




### Identifying ARIMA Parameters
#### Identify d
1. Test for stationarity. We can use the Augmented Dickey-Fuller test, then apply differencing if the data is not stationary.
2. Start with d=1, then increase the order of differencing until the data is stationary.

#### Identify q
1. Create the ACF(Autocorrelation Function) plot.
2. The ACF plot shows the correlation between the current value of the time series and its past values(autocorrelation), which tells us how the past values influence current value at various lags. Significant spikes at certain lags indicate the presense of correlations at those lags. 
3. Look for the lag at which the ACF plot shows a significant drop, use the lag as the q value.

> The drop means that the past values are not influencing the current value as much, which indicates the end of the MA(q) term.

#### Identify p
1. Create the PACF(Partial Autocorrelation Function) plot.
2. The PACF plot measures the correlation between the current value and the lags not explained by the intermediate lags.
3. Look for the lag at which the PACF plot shows a significant drop, use the lag as the p value.

> The drop means that the past values are not influencing the current value as much, which indicates the end of the AR(p) term.



