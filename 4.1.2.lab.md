## Lab Solution Code
```python
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load data
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['MedHouseVal'] = data.target

# Check for and handle missing values (if any)
if df.isnull().any().any():
    df.fillna(df.median(), inplace=True)

# Different train/test splits
splits = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]  
# Corresponds to 80/20, 70/30, 60/40 splits

results = []

for split in splits:
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(df[data.feature_names], df['MedHouseVal'], test_size=split, random_state=42)
    
    # Train linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Predict on the testing set
    predictions = model.predict(X_test)
    
    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test, predictions)
    
    results.append(f"Train/Test Split {int((1-split)*100)}/{int(split*100)}: MSE = {mse:.4f}")

# Result 1 
for result in results:
    print(result)

```

## Result interpretation
### Result 1
```python
Train/Test Split 80/20: MSE = 0.5559
Train/Test Split 70/30: MSE = 0.5306
Train/Test Split 60/40: MSE = 0.5436
Train/Test Split 50/50: MSE = 0.5309
Train/Test Split 40/60: MSE = 0.5305
Train/Test Split 30/70: MSE = 0.5275
Train/Test Split 19/80: MSE = 0.5218
```
The Mean Squared Error values from the different train/test splits are quite close to each other, which can be due to the nature of the dataset. This can due to 

### Result 2
```python
Dataset Shape: (20640, 9)

Dataset Info:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   MedInc       20640 non-null  float64
 1   HouseAge     20640 non-null  float64
 2   AveRooms     20640 non-null  float64
 3   AveBedrms    20640 non-null  float64
 4   Population   20640 non-null  float64
 5   AveOccup     20640 non-null  float64
 6   Latitude     20640 non-null  float64
 7   Longitude    20640 non-null  float64
 8   MedHouseVal  20640 non-null  float64
dtypes: float64(9)
memory usage: 1.4 MB
None
```
Observee that the size of the dataset is (20640, 9) with no null values, which makes it a fairly large dataset. 

### Result 3
```python
Descriptive Statistics:
             MedInc      HouseAge      AveRooms     AveBedrms    Population  \
count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   
mean       3.870671     28.639486      5.429000      1.096675   1425.476744   
std        1.899822     12.585558      2.474173      0.473911   1132.462122   
min        0.499900      1.000000      0.846154      0.333333      3.000000   
25%        2.563400     18.000000      4.440716      1.006079    787.000000   
50%        3.534800     29.000000      5.229129      1.048780   1166.000000   
75%        4.743250     37.000000      6.052381      1.099526   1725.000000   
max       15.000100     52.000000    141.909091     34.066667  35682.000000   

           AveOccup      Latitude     Longitude   MedHouseVal  
count  20640.000000  20640.000000  20640.000000  20640.000000  
mean       3.070655     35.631861   -119.569704      2.068558  
std       10.386050      2.135952      2.003532      1.153956  
min        0.692308     32.540000   -124.350000      0.149990  
25%        2.429741     33.930000   -121.800000      1.196000  
50%        2.818116     34.260000   -118.490000      1.797000  
75%        3.282261     37.710000   -118.010000      2.647250  
max     1243.333333     41.950000   -114.310000      5.000010  
```
No extreme outliers are observed in the dataset by observing Mean and Std values. 

## Conclusion
Recall that how MSE is calculated:
$$MSE = \frac{1}{N}(y_i - \hat{y}_i)^2$$

Stable values of Mean Squared Error:
1. Large and Homogeneous dataset with no extreme outliers
3. Consisttent feature distribution
4. Good train/test split ratios