## Objective of Clustering
- Separate the data into K  clusters
- Identify the optimal number of clusters so that each member in the cluster are actually alike

## Math Behind K-Means
We assign a center(centroid) to each cluster, for $K$ clusters in total. We then calculate the distance between each member and the centroid. A good clustering should have small within-cluster variation and large between-cluster variation.
### Within-Cluster Variation
The goal of making each member in a cluster similar to each other is achieved by minimizing the within-cluster variation.

We denote the within cluster variation as:
$$\sum_{1\leq i \leq K} \sum_{x \in C_i} d(x, \mu_i)$$
where:
- $K$ is the number of clusters
- $C_i$ is the $i^{th}$ cluster
- $d(x, \mu_i)$ is the distance between the observation $x$ and the centroid of the $i^{th}$ cluster

### Between-Cluster Variation
The goal of making each cluster different from each other is achieved by maximizing the between-cluster variation.

We denote the between cluster variation as:
$$\sum_{1\leq i \leq K} \sum_{1\leq j \leq K} d(\mu_i, \mu_j)$$
where:
- $K$ is the number of clusters
- $\mu_i$ is the centroid of the $i^{th}$ cluster
- $d(\mu_i, \mu_j)$ is the distance between the centroid of the $i^{th}$ cluster and the centroid of the $j^{th}$ cluster

> Essentially by choosing the closest centroid for all the data poitns, we ensure that all the members in the cluster are also close to each other, thus ensure the cluster is homogenous.


### K-Means Algorithm
As finding the optimal number of clusters is a difficult problem, we often use the K-Means algorithm to find the clusters. The $K$ is pre-specified by the user.

The practical K-Means algorithm works as follows:
1. Randomly choose $K$ data points as the initial centroids
2. Calculate for each data point, the distance to each centroid, and assign the data point to the closest centroid
3. After all data points are assigned to a cluster, calculate the new centroid for each cluster, which is the mean of all the data points in the cluster
4. Repeat steps 2 and 3 until the centroids do not change significantly

The algorithm is guaranteed to converge(stop running with a result), but it may not converge to the global minimum(optimal solution). 

> K-Means algorithm is just a heuristic, and it may not always give the optimal solution. The algorithm is sensitive to the initial random centroids assignment, and the result may vary depending on the initial centroids.

## Challenge
The challenge of K-Means is to find the optimal number of clusters. Shall we keep 2, 3, ... or 10 clusters? Too few clusters may not capture the underlying structure of the data, while too many clusters may give irrelevant clusters.

### Elbow Method
The Elbow Method is a common method to find the optimal number of clusters. The method plots the within-cluster variation against the number of clusters, and the optimal number of clusters is the "elbow" point where we observe a less steep slope of decrease for the within-cluster variation, which may indicate that adding more number of clusters will not significantly improve the clustering.
